{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones de jornadas de fases con AA (Aprendizaje Automático)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familia de algoritmos: <code>XGBoost</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de las bibliotecas de AA en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import uuid\n",
    "import math\n",
    "import sys\n",
    "import csv\n",
    "import sqlite3\n",
    "import json\n",
    "!conda install --yes --prefix {sys.prefix} prettytable xgboost graphviz python-graphviz mscorefonts\n",
    "import xgboost as xgb\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_manager._rebuild()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE_THRESHOLD = 10\n",
    "EPOCHS = 1000\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))\n",
    "    \n",
    "\n",
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters\")\n",
    "            print(json.dumps(results['params'][candidate], indent=4, sort_keys=True))\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "def get_min_value(predicted_value, phase_number):\n",
    "    \n",
    "    \n",
    "    def min_phase_val(i):\n",
    "        switcher = {\n",
    "                1: 0.75,\n",
    "                2: 1.5,\n",
    "                3: 2,\n",
    "                4: 0.25\n",
    "             }\n",
    "        return switcher.get(i,f\"Invalid phase:{i}\")\n",
    "\n",
    "\n",
    "    rounded_val = round((float(predicted_value)*4))/4\n",
    "    if rounded_val > 0 and rounded_val > min_phase_val(phase_number):\n",
    "        return rounded_val\n",
    "             \n",
    "    return min_phase_val(phase_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los datos para entreñar y testar el modelo de AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv data sets\n",
    "csv_dataset = pd.read_csv(\"train/train-offers-dataset.csv\")\n",
    "\n",
    "# print the first few rows to make sure that data has been loaded as expected\n",
    "csv_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar los datos de entreño-test; quitar funciones, normalizar y asignar escaladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with low occurance/correlation\n",
    "df_train_test_base = pd.DataFrame(csv_dataset)\n",
    "\n",
    "# shuffle the data\n",
    "df_train_test_base_shuffled = shuffle(df_train_test_base, random_state=0)\n",
    "\n",
    "# phaseprediction test train split\n",
    "df_train_test_phase1 = df_train_test_base_shuffled.drop(['phase2prediction', 'phase3prediction', 'phase4prediction'], axis = 1)\n",
    "phase1_x_train = df_train_test_phase1[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered']]\n",
    "phase1_y_train = df_train_test_phase1['phase1prediction']\n",
    "\n",
    "df_train_test_phase2 = df_train_test_base_shuffled.drop(['phase3prediction', 'phase4prediction'], axis = 1) \n",
    "phase2_x_train = df_train_test_phase2[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction']]\n",
    "phase2_y_train = df_train_test_phase2['phase2prediction']\n",
    "\n",
    "df_train_test_phase3 = df_train_test_base_shuffled.drop(['phase4prediction'], axis = 1)\n",
    "phase3_x_train = df_train_test_phase3[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction']]\n",
    "phase3_y_train = df_train_test_phase3['phase3prediction']\n",
    "\n",
    "df_train_test_phase4 = df_train_test_base_shuffled.copy()\n",
    "phase4_x_train = df_train_test_phase4[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction', 'phase3prediction']]\n",
    "phase4_y_train = df_train_test_phase4['phase4prediction']\n",
    "\n",
    "df_train_test_base_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los datos para validar el modelo de AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv data sets\n",
    "csv_datavalidation = pd.read_csv(\"validation/validation-offers-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar los datos de validación; quitar funciones, normalizar y asignar escaladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features with low occurance/correlation\n",
    "df_validation_base = pd.DataFrame(csv_datavalidation)\n",
    "          \n",
    "# shuffle the data\n",
    "df_validation_base_shuffled = shuffle(df_validation_base, random_state=0)\n",
    "\n",
    "# phaseprediction test train split\n",
    "df_validation_phase1 = df_validation_base_shuffled.drop(['phase2prediction', 'phase3prediction', 'phase4prediction'], axis = 1)\n",
    "phase1_x_validation = df_validation_phase1[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered']]\n",
    "phase1_y_validation = df_validation_phase1['phase1prediction']\n",
    "\n",
    "df_validation_phase2 = df_validation_base_shuffled.drop(['phase3prediction', 'phase4prediction'], axis = 1) \n",
    "phase2_x_validation = df_validation_phase2[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction']]\n",
    "phase2_y_validation = df_validation_phase2['phase2prediction']\n",
    "\n",
    "df_validation_phase3 = df_validation_base_shuffled.drop(['phase4prediction'], axis = 1)\n",
    "phase3_x_validation = df_validation_phase3[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction']]\n",
    "phase3_y_validation = df_validation_phase3['phase3prediction']\n",
    "\n",
    "df_validation_phase4 = df_validation_base_shuffled.copy()\n",
    "phase4_x_validation = df_validation_phase4[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction', 'phase3prediction']]\n",
    "phase4_y_validation = df_validation_phase4['phase4prediction']\n",
    "\n",
    "df_validation_phase1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid for the evaluation run\n",
    "eval_id = uuid.uuid4()\n",
    "\n",
    "# datetime of the evaluation run\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# results list\n",
    "results_list = []\n",
    "\n",
    "def k_fold_score_model(phase_number, phase_name, X_train, Y_train):\n",
    "    print(f\"Evaluating model for phase: {phase_number}: {phase_name}\")\n",
    "    seed = 7\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        booster=\"gbtree\", \n",
    "        n_jobs=6, \n",
    "        verbosity=0, \n",
    "        n_estimators=EPOCHS, \n",
    "        importance_type=\"weight\", \n",
    "        random_state=42, \n",
    "        eval_metric=\"mae\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Scoring neg_mean_absolute_error for phase: {phase_number}: {phase_name}\")\n",
    "    scoring_mae = 'neg_mean_absolute_error'\n",
    "    result_mae = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_mae)\n",
    "    \n",
    "    print(f\"Scoring neg_mean_squared_error for phase: {phase_number}: {phase_name}\")\n",
    "    scoring_mse = 'neg_mean_squared_error'\n",
    "    result_mse = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_mse)\n",
    "\n",
    "    print(f\"Scoring r2 (r squared) for phase: {phase_number}: {phase_name}\")\n",
    "    scoring_r2 = 'r2'\n",
    "    result_r2 = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_r2)\n",
    "    \n",
    "    score_results = [eval_id,now,phase_number,phase_name,result_mse.mean(),result_mse.std(),result_mae.mean(),result_mae.std(),result_r2.mean(),result_r2.std()]\n",
    "    results_list.append(score_results)\n",
    "\n",
    "\n",
    "# execute ML\n",
    "k_fold_score_model(\"1\", \"Recopilacion\", phase1_x_train, phase1_y_train)\n",
    "k_fold_score_model(\"2\", \"Diseno\", phase2_x_train, phase2_y_train)\n",
    "k_fold_score_model(\"3\", \"Implantacion\", phase3_x_train, phase3_y_train)\n",
    "k_fold_score_model(\"4\", \"Soporte\", phase4_x_train, phase4_y_train)\n",
    "\n",
    "# write the text and csv results\n",
    "df_score_results = pd.DataFrame(results_list, columns = ['evaluation_id','datetime','phasenumber','phasename','mse_mean','mse_std','mae_mean','mae_std','r2_mean','r2_std'])\n",
    "\n",
    "with open(\"analysis/model/xgboost/model-evaluation.csv\", \"a+\") as csv_file:\n",
    "    df_score_results.to_csv(csv_file, header=False, index=False)\n",
    "    \n",
    "df_score_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscar los hyperparametros optimales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_optimal_hyperparameters(phase_number, phase_name, x_train, y_train):\n",
    "    print(f\"Searching optimal hyperparameters for phase: {phase_number}: {phase_name}\")\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "    params = {\n",
    "        \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "        \"gamma\": uniform(0, 0.5),\n",
    "        \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "        \"max_depth\": randint(2, 6), # default 3\n",
    "        \"subsample\": uniform(0.6, 0.4)\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=3, n_jobs=4, return_train_score=True)\n",
    "\n",
    "    print(f\"Fitting the data for phase: {phase_number}: {phase_name}\")\n",
    "    search.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"Reporting best scores for phase: {phase_number}: {phase_name}\")\n",
    "    report_best_scores(search.cv_results_, 1)\n",
    "\n",
    "  \n",
    "# execute ML\n",
    "search_optimal_hyperparameters(\"1\", \"Recopilacion\", phase1_x_train, phase1_y_train)\n",
    "search_optimal_hyperparameters(\"2\", \"Diseno\", phase2_x_train, phase2_y_train)\n",
    "search_optimal_hyperparameters(\"3\", \"Implantacion\", phase3_x_train, phase3_y_train)\n",
    "search_optimal_hyperparameters(\"4\", \"Soporte\", phase4_x_train, phase4_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinar el mejor model utilizando early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_iterations = {}\n",
    "\n",
    "\n",
    "def determine_early_stopping(phase_number, phase_name, x_train, y_train):\n",
    "    print(f\"Determine best early stopping run for phase: {phase_number}: {phase_name}\")\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        booster=\"gbtree\", \n",
    "        n_jobs=4, \n",
    "        verbosity=0, \n",
    "        n_estimators=EPOCHS, \n",
    "        importance_type=\"weight\", \n",
    "        random_state=42, \n",
    "        eval_metric=\"mae\"\n",
    "    )\n",
    "\n",
    "    X_axis_train, X_axis_test, y_axis_train, y_axis_test = train_test_split(x_train, y_train, random_state=42)\n",
    "\n",
    "    xgb_model.fit(X_axis_train, y_axis_train, early_stopping_rounds=PATIENCE_THRESHOLD, eval_set=[(X_axis_test, y_axis_test)])\n",
    "\n",
    "    y_pred = xgb_model.predict(X_axis_test)\n",
    "\n",
    "    print(\"best score: {0}, best iteration: {1}, best ntree limit {2}\".format(xgb_model.best_score, xgb_model.best_iteration, xgb_model.best_ntree_limit))\n",
    "    best_model_iterations[phase_number] = xgb_model.best_iteration + 1\n",
    "    print(\"**************************************\")\n",
    "\n",
    "  \n",
    "# execute ML\n",
    "determine_early_stopping(\"1\", \"Recopilacion\", phase1_x_train, phase1_y_train)\n",
    "determine_early_stopping(\"2\", \"Diseno\", phase2_x_train, phase2_y_train)\n",
    "determine_early_stopping(\"3\", \"Implantacion\", phase3_x_train, phase3_y_train)\n",
    "determine_early_stopping(\"4\", \"Soporte\", phase4_x_train, phase4_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar el modelo y guardarlo al disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(phase_number, phase_name, phase_params, phase_predictor, x_train, y_train):\n",
    "    print(f\"Save best model for phase: {phase_number}: {phase_name}\")\n",
    "    print(f\"Column number: {x_train.num_col()}\")\n",
    "    print(f\"Best number of rounds: {best_model_iterations[phase_number]} for phase: {phase_number}: {phase_name}\")\n",
    "    best_model = xgb.train(\n",
    "        phase_params, \n",
    "        x_train, \n",
    "        num_boost_round=best_model_iterations[phase_number], \n",
    "        evals=[(y_train, phase_predictor)]\n",
    "    )\n",
    "    \n",
    "    xgb.plot_importance(best_model)\n",
    "\n",
    "    # converts the target tree to a graphviz instance\n",
    "    xgb.to_graphviz(best_model, num_trees=best_model.best_iteration)\n",
    "    \n",
    "    print(f\"Saving model to phase_{phase_number}_model.model\")\n",
    "    best_model.save_model(f\"models-saved/xgboost/phase_{phase_number}_model.model\")\n",
    "    \n",
    "\n",
    "# model 1\n",
    "phase1_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"importance_type\": \"weight\",\n",
    "    \"n_jobs\": 6,\n",
    "    \"random_state\": 42, \n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"colsample_bytree\": 0.7992694074557947,\n",
    "    \"gamma\": 0.03177917514301182,\n",
    "    \"learning_rate\": 0.12329469651469865,\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.8918424713352255\n",
    "}\n",
    "\n",
    "# model 2\n",
    "phase2_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"importance_type\": \"weight\",\n",
    "    \"n_jobs\": 6, \n",
    "    \"random_state\": 42, \n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"colsample_bytree\": 0.796805916483275,\n",
    "    \"gamma\": 0.02170039164908638,\n",
    "    \"learning_rate\": 0.30739299906707884,\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.7011960671956965\n",
    "}\n",
    "\n",
    "# model 3\n",
    "phase3_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"importance_type\": \"weight\",\n",
    "    \"n_jobs\": 6,\n",
    "    \"random_state\": 42, \n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"colsample_bytree\": 0.796805916483275,\n",
    "    \"gamma\": 0.02170039164908638,\n",
    "    \"learning_rate\": 0.30739299906707884,\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.7011960671956965\n",
    "}\n",
    "\n",
    "# model 4\n",
    "phase4_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"importance_type\": \"weight\",\n",
    "    \"n_jobs\": 6,\n",
    "    \"random_state\": 42, \n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"colsample_bytree\": 0.9248848696025762,\n",
    "    \"gamma\": 0.01523607636679708,\n",
    "    \"learning_rate\": 0.290164494322417,\n",
    "    \"max_depth\": 3,\n",
    "    \"subsample\": 0.758865533001633\n",
    "}\n",
    "\n",
    "# execute ML\n",
    "X_dmatrix_train_phase1, X_dmatrix_test_phase1, y_dmatrix_train_phase1, y_dmatrix_test_phase1 = train_test_split(phase1_x_train,phase1_y_train,test_size=.3, random_state=42)\n",
    "dtrain_phase1 = xgb.DMatrix(X_dmatrix_train_phase1, label=y_dmatrix_train_phase1)\n",
    "dtest_phase1 = xgb.DMatrix(X_dmatrix_test_phase1, label=y_dmatrix_test_phase1)\n",
    "save_best_model(\"1\", \"Recopilacion\", phase1_params, \"phase1prediction\", dtrain_phase1, dtest_phase1)\n",
    "\n",
    "X_dmatrix_train_phase2, X_dmatrix_test_phase2, y_dmatrix_train_phase2, y_dmatrix_test_phase2 = train_test_split(phase2_x_train,phase2_y_train,test_size=.3, random_state=42)\n",
    "dtrain_phase2 = xgb.DMatrix(X_dmatrix_train_phase2, label=y_dmatrix_train_phase2)\n",
    "dtest_phase2 = xgb.DMatrix(X_dmatrix_test_phase2, label=y_dmatrix_test_phase2)\n",
    "save_best_model(\"2\", \"Diseno\", phase2_params, \"phase2prediction\", dtrain_phase2, dtest_phase2)\n",
    "\n",
    "X_dmatrix_train_phase3, X_dmatrix_test_phase3, y_dmatrix_train_phase3, y_dmatrix_test_phase3 = train_test_split(phase3_x_train,phase3_y_train,test_size=.3, random_state=42)\n",
    "dtrain_phase3 = xgb.DMatrix(X_dmatrix_train_phase3, label=y_dmatrix_train_phase3)\n",
    "dtest_phase3 = xgb.DMatrix(X_dmatrix_test_phase3, label=y_dmatrix_test_phase3)\n",
    "save_best_model(\"3\", \"Implantacion\", phase3_params, \"phase3prediction\", dtrain_phase3, dtest_phase3)\n",
    "\n",
    "X_dmatrix_train_phase4, X_dmatrix_test_phase4, y_dmatrix_train_phase4, y_dmatrix_test_phase4 = train_test_split(phase4_x_train,phase4_y_train,test_size=.3, random_state=42)\n",
    "dtrain_phase4 = xgb.DMatrix(X_dmatrix_train_phase4, label=y_dmatrix_train_phase4)\n",
    "dtest_phase4 = xgb.DMatrix(X_dmatrix_test_phase4, label=y_dmatrix_test_phase4)\n",
    "save_best_model(\"4\", \"Soporte\", phase4_params, \"phase4prediction\", dtrain_phase4, dtest_phase4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predecir las jornadas por fase con los datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid for the evaluation run\n",
    "eval_id = uuid.uuid4()\n",
    "\n",
    "# datetime of the evaluation run\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# results list\n",
    "results_list = []\n",
    "\n",
    "\n",
    "def predict_phases(phase_number, phase_name, phase_predictions, X_validation):\n",
    "    print(f\"Phase predictions for phase: {phase_number}: {phase_name}\")\n",
    "    RESULT_TYPE_OVER_ESTIMATE = \"SOBRE_ESTIMACION\"\n",
    "    RESULT_TYPE_UNDER_ESTIMATE = \"SUB_ESTIMACION\"\n",
    "    RESULT_TYPE_EQUAL_ESTIMATE = \"EQUAL_ESTIMACION\"\n",
    "    \n",
    "    loaded_model = xgb.Booster()\n",
    "    loaded_model.load_model(f\"models-saved/xgboost/phase_{phase_number}_model.model\")\n",
    "    \n",
    "    # get predictions based on validation data set\n",
    "    predictors = loaded_model.predict(X_validation)\n",
    "    \n",
    "    # enumerate the predictions\n",
    "    for idx, j in enumerate(predictors):\n",
    "        predicted = get_min_value(j, phase_number)\n",
    "        actual = phase_predictions[idx]\n",
    "        loss = 0\n",
    "        percentage = 0\n",
    "        result_type = RESULT_TYPE_EQUAL_ESTIMATE\n",
    "        \n",
    "        # OVERestimate\n",
    "        if predicted > actual:\n",
    "            loss = predicted - actual\n",
    "            if loss > 0 and actual > 0:\n",
    "                percentage = (loss/predicted)*100\n",
    "            result_type = RESULT_TYPE_OVER_ESTIMATE\n",
    "        # UNDERestimate\n",
    "        elif actual > predicted:\n",
    "            loss = actual - predicted\n",
    "            if loss > 0 and actual > 0:\n",
    "                percentage = ((loss/actual)*100) * -1\n",
    "            \n",
    "            result_type = RESULT_TYPE_UNDER_ESTIMATE\n",
    "\n",
    "        predict_result = [eval_id, idx, now,phase_number, phase_name, actual, predicted, loss, percentage, result_type]\n",
    "        results_list.append(predict_result)\n",
    "            \n",
    "\n",
    "# execute predictions using the validation dataset\n",
    "dvalidation_phase1 = xgb.DMatrix(phase1_x_validation, label=phase1_y_validation)\n",
    "predict_phases(1, \"Recopilacion\", phase1_y_validation, dvalidation_phase1)\n",
    "\n",
    "dvalidation_phase2 = xgb.DMatrix(phase2_x_validation, label=phase2_y_validation)\n",
    "predict_phases(2, \"Diseno\", phase2_y_validation, dvalidation_phase2)\n",
    "\n",
    "dvalidation_phase3 = xgb.DMatrix(phase3_x_validation, label=phase3_y_validation)\n",
    "predict_phases(3, \"Implantacion\", phase3_y_validation, dvalidation_phase3)\n",
    "\n",
    "dvalidation_phase4 = xgb.DMatrix(phase4_x_validation, label=phase4_y_validation)\n",
    "predict_phases(4, \"Soporte\", phase4_y_validation, dvalidation_phase4)\n",
    "\n",
    "# write the text and csv result\n",
    "df_prediction_results = pd.DataFrame(results_list, columns = ['evaluation_id','correlation_id','datetime','phasenumber','phasename','actual','predicted','loss','percentage', 'result_type']).sort_values('correlation_id', ascending=True)\n",
    "\n",
    "model_predictions_csv = f\"analysis/predictions/xgboost/model-predictions_{eval_id}.csv\"\n",
    "with open(model_predictions_csv, \"w+\") as csv_file:\n",
    "    df_prediction_results.to_csv(csv_file, header=True, index=False)\n",
    "print(\"\\n\")\n",
    "print(f\"Evaluatiuon id: {eval_id}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "df_prediction_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\":memory:\")\n",
    "df_model_predictions = pd.read_csv(model_predictions_csv)\n",
    "df_model_predictions.to_sql(\"predictions\", conn, if_exists='append', index=False)\n",
    "\n",
    "query_details = [ {\"phasenumber\": 1, \"phasename\": \"Recopilacion\", \"min_val\": -0.5, \"max_val\": 0.5}, \n",
    "                  {\"phasenumber\": 2, \"phasename\": \"Diseno\", \"min_val\": -0.75, \"max_val\": 0.75},\n",
    "                  {\"phasenumber\": 3, \"phasename\": \"Implantacion\", \"min_val\": -0.75, \"max_val\": 0.75},\n",
    "                  {\"phasenumber\": 4, \"phasename\": \"Soporte\", \"min_val\": -0.50, \"max_val\": 0.50}\n",
    "                ]\n",
    "\n",
    "accum_acceptable_offers = 0\n",
    "accum_precise_offers = 0\n",
    "accum_non_acceptable_offers = 0\n",
    "accum_non_acceptable_under = 0\n",
    "accum_non_acceptable_over = 0\n",
    "accum_total_offers = 0\n",
    "\n",
    "# uuid for the evaluation run\n",
    "eval_id = uuid.uuid4()\n",
    "\n",
    "# datetime of the evaluation run\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "phase_analysis_results = []\n",
    "    \n",
    "for detail in query_details:\n",
    "    print(\"####################################################################################\")\n",
    "    print(\"Estadisticas de fase {}: {}\".format(detail['phasenumber'], detail['phasename']))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    phase_query_ofertas_precise = \"SELECT COUNT(correlation_id) AS ofertas_precise FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) = 0)\".format(detail['phasenumber'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_precise_offers = pd.read_sql_query(phase_query_ofertas_precise, conn)\n",
    "    \n",
    "    phase_query_ofertas_aceptable = \"SELECT COUNT(correlation_id) AS ofertas_aceptable FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) >= {} AND (actual_std-predicted_std) <= {})\".format(detail['phasenumber'], detail['min_val'], detail['max_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_acceptable_offers = pd.read_sql_query(phase_query_ofertas_aceptable, conn)\n",
    "\n",
    "    phase_query_ofertas_noaceptable = \"SELECT COUNT(correlation_id) AS ofertas_no_aceptable FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) < {} OR (actual_std-predicted_std) > {})\".format(detail['phasenumber'], detail['min_val'], detail['max_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_non_acceptable_offers = pd.read_sql_query(phase_query_ofertas_noaceptable, conn)\n",
    "    \n",
    "    phase_query_ofertas_under = \"SELECT COUNT(correlation_id) AS sub_estimaciones FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) > {})\".format(detail['phasenumber'], detail['max_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_non_acceptable_under = pd.read_sql_query(phase_query_ofertas_under, conn)\n",
    "    \n",
    "    phase_query_ofertas_over = \"SELECT COUNT(correlation_id) AS sobre_estimaciones FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) < {})\".format(detail['phasenumber'], detail['min_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_non_acceptable_over = pd.read_sql_query(phase_query_ofertas_over, conn)\n",
    "    \n",
    "    precise_offers = df_precise_offers['ofertas_precise'].iloc[0]\n",
    "    acceptable_offers = (df_acceptable_offers['ofertas_aceptable'].iloc[0]) - precise_offers\n",
    "    non_acceptable_offers = df_non_acceptable_offers['ofertas_no_aceptable'].iloc[0]\n",
    "    total_offers = precise_offers + acceptable_offers + non_acceptable_offers\n",
    "    acceptable_percentage = \"{:.2f}\".format(((precise_offers + acceptable_offers)/total_offers)*100)\n",
    "    margin_error = detail['max_val'] * 8\n",
    "\n",
    "    under_estimations = df_non_acceptable_under['sub_estimaciones'].iloc[0]\n",
    "    over_estimations = df_non_acceptable_over['sobre_estimaciones'].iloc[0]\n",
    "    \n",
    "    accum_acceptable_offers = accum_acceptable_offers + acceptable_offers\n",
    "    accum_non_acceptable_offers = accum_non_acceptable_offers + non_acceptable_offers\n",
    "    accum_precise_offers = accum_precise_offers + precise_offers\n",
    "    accum_non_acceptable_under = accum_non_acceptable_under + under_estimations\n",
    "    accum_non_acceptable_over = accum_non_acceptable_over + over_estimations\n",
    "    accum_total_offers = accum_total_offers + total_offers\n",
    "    \n",
    "    table = PrettyTable(['ofertas total','ofertas precisas','ofertas aceptable','ofertas no aceptable','aceptable %','margen error horas +/-','sub estimaciones','sobre estimaciones'])\n",
    "    table.add_row([\n",
    "        total_offers,\n",
    "        precise_offers,\n",
    "        acceptable_offers, \n",
    "        non_acceptable_offers,\n",
    "        acceptable_percentage+\"%\",\n",
    "        margin_error,\n",
    "        under_estimations,\n",
    "        over_estimations\n",
    "    ])\n",
    "\n",
    "    print(table)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    phase_query_ofertas = \"SELECT correlation_id AS oferta, phasenumber, phasename, actual_std AS actual, predicted_std As predicted, actual_std-predicted_std As diff, printf('%.2f', CASE WHEN predicted_std > actual_std THEN ((predicted_std-actual_std)/predicted_std)*100 WHEN actual_std > predicted_std THEN ((actual_std-predicted_std)/actual_std)*100*-1 ELSE 0 END) AS 'diff %' FROM predictions WHERE phasenumber = {} ORDER BY DIFF ASC\".format(detail['phasenumber'])\n",
    "    print(\"POR OFERTA: fase {} - {}\".format(detail['phasenumber'], detail['phasename']))\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_prediction_by_offer = pd.read_sql_query(phase_query_ofertas, conn)\n",
    "    print(pd.DataFrame(df_prediction_by_offer))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    validation_offer_results_csv = \"analysis/predictions/linearlearner/validation-metrics-offers-{}.csv\".format(eval_id)\n",
    "    with open(validation_offer_results_csv, \"a+\") as csv_file:\n",
    "        pd.DataFrame(df_prediction_by_offer).to_csv(csv_file, header=False, index=False)\n",
    "    \n",
    "    phase_data = [eval_id, now, detail['phasenumber'], detail['phasename'], total_offers, precise_offers, acceptable_offers, non_acceptable_offers, acceptable_percentage, margin_error, under_estimations, over_estimations]\n",
    "    phase_analysis_results.append(phase_data)\n",
    "  \n",
    "\n",
    "print(\"####################################################################################\")\n",
    "print(\"Totales de todas las fases\")\n",
    "accum_table = PrettyTable(['fases total','fases precisa','fases aceptable','fases no aceptable','aceptable %','sub estimaciones','sobre estimaciones'])\n",
    "\n",
    "accum_acceptable_percentage = \"{:.2f}\".format(((accum_precise_offers + accum_acceptable_offers)/accum_total_offers)*100)\n",
    "accum_table.add_row([\n",
    "    accum_total_offers, \n",
    "    accum_precise_offers,\n",
    "    accum_acceptable_offers, \n",
    "    accum_non_acceptable_offers,\n",
    "    accum_acceptable_percentage,\n",
    "    accum_non_acceptable_under,\n",
    "    accum_non_acceptable_over\n",
    "])\n",
    "\n",
    "print(accum_table)\n",
    "print(\"\\n\")\n",
    "    \n",
    "phase_query_ofertas = \"SELECT correlation_id AS oferta, phasenumber, phasename, actual_std AS actual, predicted_std As predicted, actual_std-predicted_std As diff, printf('%.2f', CASE WHEN predicted_std > actual_std THEN ((predicted_std-actual_std)/predicted_std)*100 WHEN actual_std > predicted_std THEN ((actual_std-predicted_std)/actual_std)*100*-1 ELSE 0 END) AS 'diff %' FROM predictions ORDER BY correlation_id, phasenumber ASC\"\n",
    "print(\"####################################################################################\")\n",
    "print(\"CADA FASE AGRUPADO POR OFERTA\")\n",
    "print(\"\\n\")\n",
    "pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "print(pd.read_sql_query(phase_query_ofertas, conn))\n",
    "print(\"\\n\")\n",
    "\n",
    "total_data = [eval_id, now, 5, 'Total', accum_total_offers, accum_precise_offers, accum_acceptable_offers, accum_non_acceptable_offers, accum_acceptable_percentage, 0, accum_non_acceptable_under, accum_non_acceptable_over]\n",
    "phase_analysis_results.append(total_data)\n",
    "df_analysis_results = pd.DataFrame(phase_analysis_results, columns = ['evaluation_id', 'datetime', 'phasenumber', 'phasename', 'ofertas_total','estimaciones_precisa','ofertas_aceptable','ofertas_no_aceptable','percentage_aceptable','margen error horas +/-','sub_estimaciones','sobre_estimaciones'])\n",
    "\n",
    "validation_results_csv = \"analysis/predictions/linearlearner/validation-metrics.csv\"\n",
    "with open(validation_results_csv, \"a+\") as csv_file:\n",
    "    df_analysis_results.to_csv(csv_file, header=False, index=False)\n",
    "    \n",
    "df_analysis_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = ['Recopilación', 'Disneo', 'Implantación', 'Soporte', 'Totales']\n",
    "precisas = df_analysis_results['estimaciones_precisa'].values\n",
    "aceptable = df_analysis_results['ofertas_aceptable'].values\n",
    "errors = df_analysis_results['ofertas_no_aceptable'].values\n",
    "\n",
    "pos = np.arange(len(phases))\n",
    "\n",
    "plt.bar(pos, precisas, width=0.8, label='precisas', color='#00b871', bottom=aceptable+errors)\n",
    "plt.bar(pos, aceptable, width=0.8, label='aceptable', color='#68fc1e', bottom=errors)\n",
    "plt.bar(pos, errors, width=0.8, label='errors', color='red')\n",
    "\n",
    "plt.xticks(pos, phases)\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.xlabel(\"Phases\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Precisión de las estimaciones\")\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion EndPoint / Invocación Ad-Hoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de oferta y invocación de predicción ad-hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#        OFFER DETAILS           #\n",
    "##################################\n",
    "\n",
    "offer = {\n",
    "  \"greenfield\": 1,          # 1 == TRUE, 0 == FALSE\n",
    "  \"vpc\": 0.5,               # 0 == 0 vpc, 0.5 == 1 vpc, 1 == 2 vpc\n",
    "  \"subnets\": 0.75,          # 0 == 0 subnets, 0.25 == 1 subnets, 0.5 == 2 subnets, 0.75 == 3 subnets, 1 == 4 subnets\n",
    "  \"connectivity\": 1,        # 1 == TRUE, 0 == FALSE \n",
    "  \"peerings\": 0,            # 1 == TRUE, 0 == FALSE \n",
    "  \"directoryservice\": 0,    # 1 == TRUE, 0 == FALSE\n",
    "  \"otherservices\": 1,     # 0 == 0 otherservices, 0.2 == 1 otherservices, 0.4 == 2 otherservices, 0.6 == 3 otherservices, 0.8 == 4 otherservices, 1 == 5 otherservices\n",
    "  \"advsecurity\": 0,         # 1 == TRUE, 0 == FALSE\n",
    "  \"advlogging\": 0,          # 1 == TRUE, 0 == FALSE\n",
    "  \"advmonitoring\": 0,       # 1 == TRUE, 0 == FALSE\n",
    "  \"advbackup\": 0,           # 1 == TRUE, 0 == FALSE\n",
    "  \"vms\": 0.3,               # 0 == 0 vms, 0.1 == 1 vms, 0.2 == 2 vms, 0.3 == 3 vms .... 0.8 == 8 vms, 0.9 == 9 vms, 1 == 10 vms\n",
    "  \"buckets\": 0.5,           # 0 == 0 buckets, 0.5 == 1 buckets, 1 == 2 buckets\n",
    "  \"databases\": 1,           # 0 == 0 BBDD, 0.5 == 1 BBDD, 1 == 2 BBDD\n",
    "  \"elb\": 0,                 # 1 == TRUE, 0 == FALSE \n",
    "  \"autoscripts\": 0,         # 1 == TRUE, 0 == FALSE \n",
    "  \"administered\": 0         # 1 == TRUE, 0 == FALSE \n",
    "}\n",
    "\n",
    "##################################\n",
    "#      OFFER DETAILS END         #\n",
    "##################################\n",
    "\n",
    "\n",
    "##################################\n",
    "# HERE BE DRAGONS!!! \n",
    "# Modify the code below \n",
    "# at your own risk\n",
    "##################################\n",
    "\n",
    "predicition_result = []\n",
    "\n",
    "def make_phase_predictions(phase_number, phase_name, offer_details):\n",
    "    loaded_model = xgb.XGBRegressor()\n",
    "    loaded_model.load_model(f\"models-saved/xgboost/phase_{phase_number}_model.model\")\n",
    "    # dvalidation_phase1 = xgb.DMatrix(phase1_x_validation, label=phase1_y_validation)\n",
    "    phase_prediction = loaded_model.predict(offer_details)\n",
    "    prediction_scaled = get_min_value(phase_prediction[0], phase_number)\n",
    "    predict_result = [phase_number, phase_name, prediction_scaled]\n",
    "    predicition_result.append(predict_result)\n",
    "    return prediction_scaled\n",
    "\n",
    "\n",
    "# phase 1 predicition\n",
    "offer_rows = {'offer': offer}\n",
    "offer_frame = pd.DataFrame.from_dict(offer_rows, orient='index')\n",
    "phase1_predicition = make_phase_predictions(1, \"Recopilacion\", offer_frame)\n",
    "\n",
    "# phase 2 predicition\n",
    "offer['phase1prediction'] = phase1_predicition\n",
    "offer_rows = {'offer': offer}\n",
    "offer_frame = pd.DataFrame.from_dict(offer_rows, orient='index')\n",
    "phase2_predicition = make_phase_predictions(2, \"Diseno\", offer_frame)\n",
    "\n",
    "# phase 3 predicition\n",
    "offer['phase2prediction'] = phase2_predicition\n",
    "offer_rows = {'offer': offer}\n",
    "offer_frame = pd.DataFrame.from_dict(offer_rows, orient='index')\n",
    "phase3_predicition = make_phase_predictions(3, \"Implantacion\", offer_frame)\n",
    "\n",
    "# phase 4 predicition\n",
    "offer['phase3prediction'] = phase3_predicition\n",
    "offer_rows = {'offer': offer}\n",
    "offer_frame = pd.DataFrame.from_dict(offer_rows, orient='index')\n",
    "phase4_predicition = make_phase_predictions(4, \"Soporte\", offer_frame)\n",
    "\n",
    "print(\"Detalles de la oferta:\\n\")\n",
    "offer['phase1prediction'] = predicition_result[0][2]\n",
    "offer['phase2prediction'] = predicition_result[1][2]\n",
    "offer['phase3prediction'] = predicition_result[2][2]\n",
    "offer['phase4prediction'] = predicition_result[3][2]\n",
    "print(json.dumps(offer, indent=4, sort_keys=True))\n",
    "\n",
    "print(\"\\nPrediccion:\\n\")\n",
    "table = PrettyTable(['# Fase','Fase','Jornadas'])\n",
    "table.align[\"# Fase\"] = \"c\"\n",
    "table.align[\"Fase\"] = \"l\"\n",
    "table.align[\"Jornadas\"] = \"c\"\n",
    "for row in predicition_result:\n",
    "    table.add_row(row)\n",
    "\n",
    "table.add_row([5, 'Total', (phase1_predicition + phase2_predicition + phase3_predicition + phase4_predicition)])\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
