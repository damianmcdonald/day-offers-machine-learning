{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones de jornadas de fases con AA (Aprendizaje Automático)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familia de algoritmos: <code>DecisionTree</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de las bibliotecas de AA en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import dump, load\n",
    "from IPython.display import display, Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import datetime\n",
    "import uuid\n",
    "import math\n",
    "import csv\n",
    "import sqlite3\n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!conda install --yes --prefix {sys.prefix} prettytable mscorefonts pydotplus\n",
    "from prettytable import PrettyTable\n",
    "import pydotplus\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_manager._rebuild()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_value(scaler_max, scaled_value, phase_number):\n",
    "    \n",
    "    \n",
    "    def min_phase_val(i):\n",
    "        switcher = {\n",
    "                1: 0.75,\n",
    "                2: 1.5,\n",
    "                3: 2,\n",
    "                4: 0.25\n",
    "             }\n",
    "        return switcher.get(i,f\"Invalid phase:{i}\")\n",
    "\n",
    "\n",
    "    val_to_round = scaled_value * scaler_max\n",
    "    rounded_val = round((float(val_to_round)*4))/4\n",
    "    if rounded_val > 0 and rounded_val > min_phase_val(phase_number):\n",
    "        return rounded_val\n",
    "             \n",
    "    return min_phase_val(phase_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los datos para entreñar y testar el modelo de AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv data sets\n",
    "csv_dataset = pd.read_csv(\"train/train-offers-dataset.csv\")\n",
    "\n",
    "# print the first few rows to make sure that data has been loaded as expected\n",
    "csv_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar los datos de entreño-test; quitar funciones, normalizar y asignar escaladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop featuresd with low occurance/correlation\n",
    "df_train_test_base = shuffle(pd.DataFrame(csv_dataset), random_state=0)\n",
    "\n",
    "# grab the prediction columns\n",
    "df_train_test_phase1_std = df_train_test_base[['phase1prediction']].copy()\n",
    "df_train_test_phase2_std = df_train_test_base[['phase2prediction']].copy()\n",
    "df_train_test_phase3_std = df_train_test_base[['phase3prediction']].copy()\n",
    "df_train_test_phase4_std = df_train_test_base[['phase4prediction']].copy()\n",
    "\n",
    "# create scalers for the prediction columns\n",
    "train_test_phase1_scaler = preprocessing.MinMaxScaler()\n",
    "train_test_phase2_scaler = preprocessing.MinMaxScaler()\n",
    "train_test_phase3_scaler = preprocessing.MinMaxScaler()\n",
    "train_test_phase4_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# scale the prediction columns\n",
    "df_train_test_phase1_scaled = pd.DataFrame(train_test_phase1_scaler.fit_transform(df_train_test_phase1_std), columns = ['phase1prediction'])\n",
    "df_train_test_phase2_scaled = pd.DataFrame(train_test_phase2_scaler.fit_transform(df_train_test_phase2_std), columns = ['phase2prediction'])\n",
    "df_train_test_phase3_scaled = pd.DataFrame(train_test_phase3_scaler.fit_transform(df_train_test_phase3_std), columns = ['phase3prediction'])\n",
    "df_train_test_phase4_scaled = pd.DataFrame(train_test_phase4_scaler.fit_transform(df_train_test_phase4_std), columns = ['phase4prediction'])\n",
    "\n",
    "# normalize the dataset\n",
    "mm_train_test_scaler = preprocessing.MinMaxScaler()\n",
    "df_train_test_base_scaled = pd.DataFrame(mm_train_test_scaler.fit_transform(df_train_test_base), columns = ['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction', 'phase3prediction', 'phase4prediction'])\n",
    "\n",
    "df_train_test_base_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los datos para validar el modelo de AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv data sets\n",
    "csv_datavalidation = pd.read_csv(\"validation/validation-offers-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar los datos de validación; quitar funciones, normalizar y asignar escaladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop featuresd with low occurance/correlation\n",
    "df_validation_base = pd.DataFrame(csv_datavalidation)\n",
    "\n",
    "# grab the prediction columns\n",
    "df_validation_phase1_std = df_validation_base[['phase1prediction']].copy()\n",
    "df_validation_phase2_std = df_validation_base[['phase2prediction']].copy()\n",
    "df_validation_phase3_std = df_validation_base[['phase3prediction']].copy()\n",
    "df_validation_phase4_std = df_validation_base[['phase4prediction']].copy()\n",
    "\n",
    "# scale the prediction columns using the training set scaling values for consitency\n",
    "df_train_test_phase1_scaled = df_validation_phase1_std.apply(lambda x: x/train_test_phase1_scaler.data_max_ if x.name == 'phase1prediction' else x)\n",
    "df_train_test_phase2_scaled = df_validation_phase2_std.apply(lambda x: x/train_test_phase2_scaler.data_max_ if x.name == 'phase2prediction' else x)\n",
    "df_train_test_phase3_scaled = df_validation_phase3_std.apply(lambda x: x/train_test_phase3_scaler.data_max_ if x.name == 'phase3prediction' else x)\n",
    "df_train_test_phase4_scaled = df_validation_phase4_std.apply(lambda x: x/train_test_phase4_scaler.data_max_ if x.name == 'phase4prediction' else x)\n",
    "\n",
    "df_validation_base['phase1prediction'] = df_train_test_phase1_scaled\n",
    "df_validation_base['phase2prediction'] = df_train_test_phase2_scaled\n",
    "df_validation_base['phase3prediction'] = df_train_test_phase3_scaled\n",
    "df_validation_base['phase4prediction'] = df_train_test_phase4_scaled\n",
    "\n",
    "# normalize the dataset\n",
    "df_validation_base_scaled = df_validation_base.copy()\n",
    "\n",
    "df_validation_base_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid for the evaluation run\n",
    "eval_id = uuid.uuid4()\n",
    "\n",
    "# datetime of the evaluation run\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# results list\n",
    "results_list = []\n",
    "\n",
    "\n",
    "def score_model(phase_number, phase_name, X_train, Y_train, phase_scaler_max):\n",
    "    seed = 7\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "    model = DecisionTreeRegressor()\n",
    "    \n",
    "    scoring_mae = 'neg_mean_absolute_error'\n",
    "    print(\"Scoring model for phase {}:{} using {} ...\".format(phase_number, phase_name, scoring_mae))\n",
    "    result_mae = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_mae)\n",
    "    \n",
    "    result_mae_inverse = get_standard_value(phase_scaler_max, result_mae.std(), phase_number)\n",
    "\n",
    "    scoring_mse = 'neg_mean_squared_error'\n",
    "    print(\"Scoring model for phase {}:{} using {} ...\".format(phase_number, phase_name, scoring_mse))\n",
    "    result_mse = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_mse)\n",
    "    \n",
    "    result_mse_inverse = get_standard_value(phase_scaler_max, result_mse.std(), phase_number)\n",
    "\n",
    "    scoring_r2 = 'r2'\n",
    "    print(\"Scoring model for phase {}:{} using {} ...\".format(phase_number, phase_name, scoring_r2))\n",
    "    result_r2 = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_r2)\n",
    "    \n",
    "    score_results = [eval_id,now,phase_number,phase_name,result_mse.mean(),result_mse.std(),result_mse_inverse,result_mae.mean(),result_mae.std(),result_mae_inverse,result_r2.mean(),result_r2.std()]\n",
    "    results_list.append(score_results)\n",
    "    \n",
    "\n",
    "# phaseprediction test train split\n",
    "df_train_test_phase1 = df_train_test_base_scaled.drop(['phase2prediction', 'phase3prediction', 'phase4prediction'], axis = 1)\n",
    "df_train_test_phase2 = df_train_test_base_scaled.drop(['phase3prediction', 'phase4prediction'], axis = 1)\n",
    "df_train_test_phase3 = df_train_test_base_scaled.drop(['phase4prediction'], axis = 1)\n",
    "df_train_test_phase4 = df_train_test_base_scaled.copy()\n",
    "\n",
    "phase1_x_train = df_train_test_phase1[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered'  ]]\n",
    "phase1_y_train = df_train_test_phase1['phase1prediction']\n",
    "\n",
    "phase2_x_train = df_train_test_phase2[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction']]\n",
    "phase2_y_train = df_train_test_phase2['phase2prediction']\n",
    "\n",
    "phase3_x_train = df_train_test_phase3[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction']]\n",
    "phase3_y_train = df_train_test_phase3['phase3prediction']\n",
    "\n",
    "phase4_x_train = df_train_test_phase4[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction', 'phase3prediction']]\n",
    "phase4_y_train = df_train_test_phase4['phase4prediction']\n",
    "\n",
    "# score the models\n",
    "score_model(1, \"Recopilacion\", phase1_x_train, phase1_y_train, train_test_phase1_scaler.data_max_)\n",
    "score_model(2, \"Diseno\", phase2_x_train, phase2_y_train, train_test_phase2_scaler.data_max_)\n",
    "score_model(3, \"Implantacion\", phase3_x_train, phase3_y_train, train_test_phase3_scaler.data_max_)\n",
    "score_model(4, \"Soporte\", phase4_x_train, phase4_y_train, train_test_phase4_scaler.data_max_)\n",
    "\n",
    "# write the text and csv results\n",
    "df_score_results = pd.DataFrame(results_list, columns = ['evaluation_id','datetime','phasenumber','phasename','mse_mean','mse_std','mse_inverse','mae_mean','mae_std','mae_inverse','r2_mean','r2_std'])\n",
    "\n",
    "with open(\"analysis/model/decisiontree/model-evaluation.csv\", \"a+\") as csv_file:\n",
    "    df_score_results.to_csv(csv_file, header=False, index=False)\n",
    "    \n",
    "df_score_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar el DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_tree(phase_number, phase_name, phase_features, X_train, Y_train):\n",
    "    print(\"\\n\")\n",
    "    print(f\"#### Graficio de decision tree de fase: {phase_number}: {phase_name} ####\")\n",
    "    print(\"\\n\")\n",
    "    dtree = DecisionTreeRegressor()\n",
    "    dtree.fit(X_train,Y_train)\n",
    "    dot_data = export_graphviz(dtree, out_file=None, \n",
    "                                        filled=True, rounded=True,\n",
    "                                        feature_names = phase_features,\n",
    "                                        special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "    display(Image(graph.create_png()))\n",
    "\n",
    "    \n",
    "# score the models\n",
    "phase1_features = ['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered']\n",
    "render_tree(1, \"Recopilacion\", phase1_features, phase1_x_train.head(n=20), phase1_y_train.head(n=20))\n",
    "\n",
    "phase2_features = ['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction']\n",
    "render_tree(2, \"Diseno\", phase2_features, phase2_x_train.head(n=20), phase2_y_train.head(n=20))\n",
    "\n",
    "phase3_features = ['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction']\n",
    "render_tree(3, \"Implantcion\", phase3_features, phase3_x_train.head(n=20), phase3_y_train.head(n=20))\n",
    "\n",
    "phase4_features = ['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction', 'phase3prediction']\n",
    "render_tree(4, \"Soporte\", phase4_features, phase4_x_train.head(n=20), phase4_y_train.head(n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predecir las jornadas por fase con los datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid for the evaluation run\n",
    "eval_id = uuid.uuid4()\n",
    "\n",
    "# datetime of the evaluation run\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# results list\n",
    "results_list = []\n",
    "\n",
    "def predict_phases(phase_number, phase_name, scaler_max, phase_predictions, X_train, Y_train, X_validation):\n",
    "    RESULT_TYPE_OVER_ESTIMATE = \"SOBRE_ESTIMACION\"\n",
    "    RESULT_TYPE_UNDER_ESTIMATE = \"SUB_ESTIMACION\"\n",
    "    RESULT_TYPE_EQUAL_ESTIMATE = \"EQUAL_ESTIMACION\"\n",
    "    \n",
    "    decisiontree_regression = DecisionTreeRegressor()\n",
    "    \n",
    "    # train and test the model\n",
    "    decisiontree_regression.fit(X_train,Y_train)\n",
    "    \n",
    "    # dump model to disk\n",
    "    dump(decisiontree_regression, f\"models-saved/decisiontree/phase_{phase_number}_model.model\")\n",
    "    \n",
    "    # get predictions based on validation data set\n",
    "    predictors = decisiontree_regression.predict(X_validation)\n",
    "    \n",
    "    # enumerate the predictions\n",
    "    for idx, j in enumerate(predictors):\n",
    "        predicted_scaled = j\n",
    "        actual_scaled = phase_predictions[idx]\n",
    "        predicted = get_standard_value(scaler_max, predicted_scaled, phase_number)\n",
    "        actual = get_standard_value(scaler_max, actual_scaled, phase_number)\n",
    "        loss = 0\n",
    "        percentage = 0\n",
    "        result_type = RESULT_TYPE_EQUAL_ESTIMATE\n",
    "        \n",
    "        # OVERestimate\n",
    "        if predicted > actual:\n",
    "            loss = predicted - actual\n",
    "            if loss > 0 and actual > 0:\n",
    "                percentage = (loss/predicted)*100\n",
    "            result_type = RESULT_TYPE_OVER_ESTIMATE\n",
    "        # UNDERestimate\n",
    "        elif actual > predicted:\n",
    "            loss = actual - predicted\n",
    "            if loss > 0 and actual > 0:\n",
    "                percentage = ((loss/actual)*100) * -1\n",
    "            \n",
    "            result_type = RESULT_TYPE_UNDER_ESTIMATE\n",
    "\n",
    "        predict_result = [eval_id, idx, now,phase_number, phase_name, actual_scaled, predicted_scaled, actual, predicted, loss, percentage, result_type]\n",
    "        results_list.append(predict_result)\n",
    "            \n",
    "\n",
    "# phaseprediction test train split\n",
    "df_train_test_phase1 = df_train_test_base_scaled.drop(['phase2prediction', 'phase3prediction', 'phase4prediction'], axis = 1)\n",
    "df_train_test_phase2 = df_train_test_base_scaled.drop(['phase3prediction', 'phase4prediction'], axis = 1)\n",
    "df_train_test_phase3 = df_train_test_base_scaled.drop(['phase4prediction'], axis = 1)\n",
    "df_train_test_phase4 = df_train_test_base_scaled.copy()\n",
    "\n",
    "phase1_x_train = df_train_test_phase1[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts','administered' ]]\n",
    "phase1_y_train = df_train_test_phase1['phase1prediction']\n",
    "phase2_x_train = df_train_test_phase2[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction']]\n",
    "phase2_y_train = df_train_test_phase2['phase2prediction']\n",
    "phase3_x_train = df_train_test_phase3[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction']]\n",
    "phase3_y_train = df_train_test_phase3['phase3prediction']\n",
    "phase4_x_train = df_train_test_phase4[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts', 'administered', 'phase1prediction', 'phase2prediction', 'phase3prediction']]\n",
    "phase4_y_train = df_train_test_phase4['phase4prediction']\n",
    "\n",
    "# phaseprediction validation\n",
    "df_validation_phase1 = df_validation_base_scaled.drop(['phase2prediction', 'phase3prediction', 'phase4prediction'], axis = 1)\n",
    "df_validation_phase2 = df_validation_base_scaled.drop(['phase3prediction', 'phase4prediction'], axis = 1)\n",
    "df_validation_phase3 = df_validation_base_scaled.drop(['phase4prediction'], axis = 1)\n",
    "df_validation_phase4 = df_validation_base_scaled.copy()\n",
    "\n",
    "phase1_x_validation = df_validation_phase1[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts','administered']]\n",
    "phase1_y_validation = df_validation_phase1['phase1prediction']\n",
    "phase2_x_validation = df_validation_phase2[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts','administered', 'phase1prediction']]\n",
    "phase2_y_validation = df_validation_phase2['phase2prediction']\n",
    "phase3_x_validation = df_validation_phase3[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts','administered', 'phase1prediction', 'phase2prediction']]\n",
    "phase3_y_validation = df_validation_phase3['phase3prediction']\n",
    "phase4_x_validation = df_validation_phase4[['greenfield','vpc', 'subnets', 'connectivity', 'peerings', 'directoryservice', 'otherservices', 'advsecurity', 'advlogging', 'advmonitoring', 'advbackup', 'vms', 'buckets', 'databases', 'elb', 'autoscripts','administered', 'phase1prediction', 'phase2prediction', 'phase3prediction']]\n",
    "phase4_y_validation = df_validation_phase4['phase4prediction']\n",
    "\n",
    "# execute predictions using the validation dataset\n",
    "\n",
    "# phase_number, phase_name, phase_scaler, phase_predictions, X_train, Y_train, X_validation\n",
    "predict_phases(1, \"Recopilacion\", train_test_phase1_scaler.data_max_,phase1_y_validation, phase1_x_train, phase1_y_train, phase1_x_validation)\n",
    "predict_phases(2, \"Diseno\", train_test_phase2_scaler.data_max_, phase2_y_validation, phase2_x_train, phase2_y_train, phase2_x_validation)\n",
    "predict_phases(3, \"Implantacion\", train_test_phase3_scaler.data_max_, phase3_y_validation, phase3_x_train, phase3_y_train, phase3_x_validation)\n",
    "predict_phases(4, \"Soporte\", train_test_phase4_scaler.data_max_, phase4_y_validation, phase4_x_train, phase4_y_train, phase4_x_validation)\n",
    "\n",
    "# write the text and csv result\n",
    "df_prediction_results = pd.DataFrame(results_list, columns = ['evaluation_id','correlation_id','datetime','phasenumber','phasename','actual_scaled','predicted_scaled','actual_std','predicted_std','loss','percentage', 'result_type']).sort_values('correlation_id', ascending=True)\n",
    "\n",
    "model_predictions_csv = f\"analysis/predictions/decisiontree/model-predictions_{eval_id}.csv\"\n",
    "with open(model_predictions_csv, \"w+\") as csv_file:\n",
    "    df_prediction_results.to_csv(csv_file, header=True, index=False)\n",
    "print(\"\\n\")\n",
    "print(f\"Evaluatiuon id: {eval_id}\")\n",
    "print(\"\\n\")\n",
    "df_prediction_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\":memory:\")\n",
    "df_model_predictions = pd.read_csv(model_predictions_csv)\n",
    "df_model_predictions.to_sql(\"predictions\", conn, if_exists='append', index=False)\n",
    "\n",
    "query_details = [ {\"phasenumber\": 1, \"phasename\": \"Recopilacion\", \"min_val\": -0.5, \"max_val\": 0.5}, \n",
    "                  {\"phasenumber\": 2, \"phasename\": \"Diseno\", \"min_val\": -0.75, \"max_val\": 0.75},\n",
    "                  {\"phasenumber\": 3, \"phasename\": \"Implantacion\", \"min_val\": -0.75, \"max_val\": 0.75},\n",
    "                  {\"phasenumber\": 4, \"phasename\": \"Soporte\", \"min_val\": -0.50, \"max_val\": 0.50}\n",
    "                ]\n",
    "\n",
    "accum_acceptable_offers = 0\n",
    "accum_precise_offers = 0\n",
    "accum_non_acceptable_offers = 0\n",
    "accum_non_acceptable_under = 0\n",
    "accum_non_acceptable_over = 0\n",
    "accum_total_offers = 0\n",
    "\n",
    "# uuid for the evaluation run\n",
    "eval_id = uuid.uuid4()\n",
    "\n",
    "# datetime of the evaluation run\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "phase_analysis_results = []\n",
    "    \n",
    "for detail in query_details:\n",
    "    print(\"####################################################################################\")\n",
    "    print(\"Estadisticas de fase {}: {}\".format(detail['phasenumber'], detail['phasename']))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    phase_query_ofertas_precise = \"SELECT COUNT(correlation_id) AS ofertas_precise FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) = 0)\".format(detail['phasenumber'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_precise_offers = pd.read_sql_query(phase_query_ofertas_precise, conn)\n",
    "    \n",
    "    phase_query_ofertas_aceptable = \"SELECT COUNT(correlation_id) AS ofertas_aceptable FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) >= {} AND (actual_std-predicted_std) <= {})\".format(detail['phasenumber'], detail['min_val'], detail['max_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_acceptable_offers = pd.read_sql_query(phase_query_ofertas_aceptable, conn)\n",
    "\n",
    "    phase_query_ofertas_noaceptable = \"SELECT COUNT(correlation_id) AS ofertas_no_aceptable FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) < {} OR (actual_std-predicted_std) > {})\".format(detail['phasenumber'], detail['min_val'], detail['max_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_non_acceptable_offers = pd.read_sql_query(phase_query_ofertas_noaceptable, conn)\n",
    "    \n",
    "    phase_query_ofertas_under = \"SELECT COUNT(correlation_id) AS sub_estimaciones FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) > {})\".format(detail['phasenumber'], detail['max_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_non_acceptable_under = pd.read_sql_query(phase_query_ofertas_under, conn)\n",
    "    \n",
    "    phase_query_ofertas_over = \"SELECT COUNT(correlation_id) AS sobre_estimaciones FROM predictions WHERE phasenumber = {} AND ((actual_std-predicted_std) < {})\".format(detail['phasenumber'], detail['min_val'])\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_non_acceptable_over = pd.read_sql_query(phase_query_ofertas_over, conn)\n",
    "    \n",
    "    precise_offers = df_precise_offers['ofertas_precise'].iloc[0]\n",
    "    acceptable_offers = (df_acceptable_offers['ofertas_aceptable'].iloc[0]) - precise_offers\n",
    "    non_acceptable_offers = df_non_acceptable_offers['ofertas_no_aceptable'].iloc[0]\n",
    "    total_offers = precise_offers + acceptable_offers + non_acceptable_offers\n",
    "    acceptable_percentage = \"{:.2f}\".format(((precise_offers + acceptable_offers)/total_offers)*100)\n",
    "    margin_error = detail['max_val'] * 8\n",
    "\n",
    "    under_estimations = df_non_acceptable_under['sub_estimaciones'].iloc[0]\n",
    "    over_estimations = df_non_acceptable_over['sobre_estimaciones'].iloc[0]\n",
    "    \n",
    "    accum_acceptable_offers = accum_acceptable_offers + acceptable_offers\n",
    "    accum_non_acceptable_offers = accum_non_acceptable_offers + non_acceptable_offers\n",
    "    accum_precise_offers = accum_precise_offers + precise_offers\n",
    "    accum_non_acceptable_under = accum_non_acceptable_under + under_estimations\n",
    "    accum_non_acceptable_over = accum_non_acceptable_over + over_estimations\n",
    "    accum_total_offers = accum_total_offers + total_offers\n",
    "    \n",
    "    table = PrettyTable(['ofertas total','ofertas precisas','ofertas aceptable','ofertas no aceptable','aceptable %','margen error horas +/-','sub estimaciones','sobre estimaciones'])\n",
    "    table.add_row([\n",
    "        total_offers,\n",
    "        precise_offers,\n",
    "        acceptable_offers, \n",
    "        non_acceptable_offers,\n",
    "        acceptable_percentage+\"%\",\n",
    "        margin_error,\n",
    "        under_estimations,\n",
    "        over_estimations\n",
    "    ])\n",
    "\n",
    "    print(table)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    phase_query_ofertas = \"SELECT correlation_id AS oferta, phasenumber, phasename, actual_std AS actual, predicted_std As predicted, actual_std-predicted_std As diff, printf('%.2f', CASE WHEN predicted_std > actual_std THEN ((predicted_std-actual_std)/predicted_std)*100 WHEN actual_std > predicted_std THEN ((actual_std-predicted_std)/actual_std)*100*-1 ELSE 0 END) AS 'diff %' FROM predictions WHERE phasenumber = {} ORDER BY DIFF ASC\".format(detail['phasenumber'])\n",
    "    print(\"POR OFERTA: fase {} - {}\".format(detail['phasenumber'], detail['phasename']))\n",
    "    pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "    df_prediction_by_offer = pd.read_sql_query(phase_query_ofertas, conn)\n",
    "    print(pd.DataFrame(df_prediction_by_offer))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    validation_offer_results_csv = \"analysis/predictions/linearlearner/validation-metrics-offers-{}.csv\".format(eval_id)\n",
    "    with open(validation_offer_results_csv, \"a+\") as csv_file:\n",
    "        pd.DataFrame(df_prediction_by_offer).to_csv(csv_file, header=False, index=False)\n",
    "    \n",
    "    phase_data = [eval_id, now, detail['phasenumber'], detail['phasename'], total_offers, precise_offers, acceptable_offers, non_acceptable_offers, acceptable_percentage, margin_error, under_estimations, over_estimations]\n",
    "    phase_analysis_results.append(phase_data)\n",
    "  \n",
    "\n",
    "print(\"####################################################################################\")\n",
    "print(\"Totales de todas las fases\")\n",
    "accum_table = PrettyTable(['fases total','fases precisa','fases aceptable','fases no aceptable','aceptable %','sub estimaciones','sobre estimaciones'])\n",
    "\n",
    "accum_acceptable_percentage = \"{:.2f}\".format(((accum_precise_offers + accum_acceptable_offers)/accum_total_offers)*100)\n",
    "accum_table.add_row([\n",
    "    accum_total_offers, \n",
    "    accum_precise_offers,\n",
    "    accum_acceptable_offers, \n",
    "    accum_non_acceptable_offers,\n",
    "    accum_acceptable_percentage,\n",
    "    accum_non_acceptable_under,\n",
    "    accum_non_acceptable_over\n",
    "])\n",
    "\n",
    "print(accum_table)\n",
    "print(\"\\n\")\n",
    "    \n",
    "phase_query_ofertas = \"SELECT correlation_id AS oferta, phasenumber, phasename, actual_std AS actual, predicted_std As predicted, actual_std-predicted_std As diff, printf('%.2f', CASE WHEN predicted_std > actual_std THEN ((predicted_std-actual_std)/predicted_std)*100 WHEN actual_std > predicted_std THEN ((actual_std-predicted_std)/actual_std)*100*-1 ELSE 0 END) AS 'diff %' FROM predictions ORDER BY correlation_id, phasenumber ASC\"\n",
    "print(\"####################################################################################\")\n",
    "print(\"CADA FASE AGRUPADO POR OFERTA\")\n",
    "print(\"\\n\")\n",
    "pd.set_option('display.max_rows', df_model_predictions.shape[0]+1)\n",
    "print(pd.read_sql_query(phase_query_ofertas, conn))\n",
    "print(\"\\n\")\n",
    "\n",
    "total_data = [eval_id, now, 5, 'Total', accum_total_offers, accum_precise_offers, accum_acceptable_offers, accum_non_acceptable_offers, accum_acceptable_percentage, 0, accum_non_acceptable_under, accum_non_acceptable_over]\n",
    "phase_analysis_results.append(total_data)\n",
    "df_analysis_results = pd.DataFrame(phase_analysis_results, columns = ['evaluation_id', 'datetime', 'phasenumber', 'phasename', 'ofertas_total','estimaciones_precisa','ofertas_aceptable','ofertas_no_aceptable','percentage_aceptable','margen error horas +/-','sub_estimaciones','sobre_estimaciones'])\n",
    "\n",
    "validation_results_csv = \"analysis/predictions/linearlearner/validation-metrics.csv\"\n",
    "with open(validation_results_csv, \"a+\") as csv_file:\n",
    "    df_analysis_results.to_csv(csv_file, header=False, index=False)\n",
    "    \n",
    "df_analysis_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = ['Recopilación', 'Disneo', 'Implantación', 'Soporte', 'Totales']\n",
    "precisas = df_analysis_results['estimaciones_precisa'].values\n",
    "aceptable = df_analysis_results['ofertas_aceptable'].values\n",
    "errors = df_analysis_results['ofertas_no_aceptable'].values\n",
    "\n",
    "pos = np.arange(len(phases))\n",
    "\n",
    "plt.bar(pos, precisas, width=0.8, label='precisas', color='#00b871', bottom=aceptable+errors)\n",
    "plt.bar(pos, aceptable, width=0.8, label='aceptable', color='#68fc1e', bottom=errors)\n",
    "plt.bar(pos, errors, width=0.8, label='errors', color='red')\n",
    "\n",
    "plt.xticks(pos, phases)\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.xlabel(\"Phases\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Precisión de las estimaciones\")\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.rcParams[\"figure.figsize\"] = [12,10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion EndPoint / Invocación Ad-Hoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de oferta y invocación de predicción ad-hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#        OFFER DETAILS           #\n",
    "##################################\n",
    "\n",
    "offer = {\n",
    "  \"greenfield\": 1,          # 1 == TRUE, 0 == FALSE\n",
    "  \"vpc\": 0.5,               # 0 == 0 vpc, 0.5 == 1 vpc, 1 == 2 vpc\n",
    "  \"subnets\": 0.75,          # 0 == 0 subnets, 0.25 == 1 subnets, 0.5 == 2 subnets, 0.75 == 3 subnets, 1 == 4 subnets\n",
    "  \"connectivity\": 1,        # 1 == TRUE, 0 == FALSE \n",
    "  \"peerings\": 0,            # 1 == TRUE, 0 == FALSE \n",
    "  \"directoryservice\": 0,    # 1 == TRUE, 0 == FALSE\n",
    "  \"otherservices\": 0.4,     # 0 == 0 otherservices, 0.2 == 1 otherservices, 0.4 == 2 otherservices, 0.6 == 3 otherservices, 0.8 == 4 otherservices, 1 == 5 otherservices\n",
    "  \"advsecurity\": 0,         # 1 == TRUE, 0 == FALSE\n",
    "  \"advlogging\": 0,          # 1 == TRUE, 0 == FALSE\n",
    "  \"advmonitoring\": 0,       # 1 == TRUE, 0 == FALSE\n",
    "  \"advbackup\": 0,           # 1 == TRUE, 0 == FALSE\n",
    "  \"vms\": 0.5,               # 0 == 0 vms, 0.1 == 1 vms, 0.2 == 2 vms, 0.3 == 3 vms .... 0.8 == 8 vms, 0.9 == 9 vms, 1 == 10 vms\n",
    "  \"buckets\": 0.5,           # 0 == 0 buckets, 0.5 == 1 buckets, 1 == 2 buckets\n",
    "  \"databases\": 0.5,         # 0 == 0 BBDD, 0.5 == 1 BBDD, 1 == 2 BBDD\n",
    "  \"elb\": 1,                 # 1 == TRUE, 0 == FALSE \n",
    "  \"autoscripts\": 0,         # 1 == TRUE, 0 == FALSE \n",
    "  \"administered\": 0         # 1 == TRUE, 0 == FALSE\n",
    "}\n",
    "\n",
    "##################################\n",
    "#      OFFER DETAILS END         #\n",
    "##################################\n",
    "\n",
    "\n",
    "##################################\n",
    "# HERE BE DRAGONS!!! \n",
    "# Modify the code below \n",
    "# at your own risk\n",
    "##################################\n",
    "\n",
    "# Order the index of the columns as strict ordering is required for prediction\n",
    "\n",
    "# Phase 1 column order\n",
    "colOrderPhase1 = [\n",
    "  'greenfield', \n",
    "  'vpc', \n",
    "  'subnets', \n",
    "  'connectivity', \n",
    "  'peerings', \n",
    "  'directoryservice', \n",
    "  'otherservices', \n",
    "  'advsecurity', \n",
    "  'advlogging', \n",
    "  'advmonitoring', \n",
    "  'advbackup', \n",
    "  'vms', \n",
    "  'buckets', \n",
    "  'databases', \n",
    "  'elb', \n",
    "  'autoscripts', \n",
    "  'administered'\n",
    "]\n",
    "\n",
    "# Phase 2 column order\n",
    "colOrderPhase2 = colOrderPhase1.copy()\n",
    "colOrderPhase2.append('phase1prediction')     \n",
    "\n",
    "# Phase 3 column order\n",
    "colOrderPhase3 = colOrderPhase2.copy()\n",
    "colOrderPhase3.append('phase2prediction')\n",
    "\n",
    "# Phase 4 column order\n",
    "colOrderPhase4 = colOrderPhase3.copy()\n",
    "colOrderPhase4.append('phase3prediction')  \n",
    "\n",
    "predicition_result = []\n",
    "\n",
    "\n",
    "def make_phase_predictions(phase_number, phase_name, offer_details, scaler_max):\n",
    "    linear_regression_ad_hoc = load(\"models-saved/linearlearner/phase_{}_model.model\".format(phase_number))\n",
    "    phase_prediction = linear_regression_ad_hoc.predict(offer_details)\n",
    "    prediction_scaled = get_standard_value(scaler_max, phase_prediction[0], phase_number)\n",
    "    predict_result = [phase_number, phase_name, prediction_scaled]\n",
    "    predicition_result.append(predict_result)\n",
    "    return prediction_scaled\n",
    "\n",
    "\n",
    "# phase 1 predicition\n",
    "pd_offer = pd.json_normalize(offer)\n",
    "offer_frame = pd_offer.reindex(columns=colOrderPhase1)   \n",
    "phase1_predicition = make_phase_predictions(1, \"Recopilacion\", offer_frame, train_test_phase1_scaler.data_max_)\n",
    "phase1_predicition_scaled = phase1_predicition / train_test_phase1_scaler.data_max_\n",
    "\n",
    "# phase 2 predicition\n",
    "offer['phase1prediction'] = phase1_predicition_scaled\n",
    "pd_offer = pd.json_normalize(offer)\n",
    "offer_frame = pd_offer.reindex(columns=colOrderPhase2) \n",
    "phase2_predicition = make_phase_predictions(2, \"Diseno\", offer_frame, train_test_phase2_scaler.data_max_)\n",
    "phase2_predicition_scaled = phase2_predicition / train_test_phase2_scaler.data_max_\n",
    "\n",
    "# phase 3 predicition\n",
    "offer['phase2prediction'] = phase2_predicition_scaled\n",
    "pd_offer = pd.json_normalize(offer)\n",
    "offer_frame = pd_offer.reindex(columns=colOrderPhase3)   \n",
    "phase3_predicition = make_phase_predictions(3, \"Implantacion\", offer_frame, train_test_phase3_scaler.data_max_)\n",
    "phase3_predicition_scaled = phase3_predicition / train_test_phase3_scaler.data_max_\n",
    "\n",
    "# phase 4 predicition\n",
    "offer['phase3prediction'] = phase3_predicition_scaled\n",
    "pd_offer = pd.json_normalize(offer)\n",
    "offer_frame = pd_offer.reindex(columns=colOrderPhase4) \n",
    "phase4_predicition = make_phase_predictions(4, \"Soporte\", offer_frame, train_test_phase4_scaler.data_max_)\n",
    "phase4_predicition_scaled = phase4_predicition / train_test_phase4_scaler.data_max_\n",
    "\n",
    "print(\"Detalles de la oferta:\\n\")\n",
    "offer['phase1prediction'] = predicition_result[0][2]\n",
    "offer['phase2prediction'] = predicition_result[1][2]\n",
    "offer['phase3prediction'] = predicition_result[2][2]\n",
    "offer['phase4prediction'] = predicition_result[3][2]\n",
    "print(json.dumps(offer, indent=4, sort_keys=True))\n",
    "\n",
    "print(\"\\nPrediccion:\\n\")\n",
    "table = PrettyTable(['# Fase','Fase','Jornadas'])\n",
    "table.align[\"# Fase\"] = \"c\"\n",
    "table.align[\"Fase\"] = \"l\"\n",
    "table.align[\"Jornadas\"] = \"c\"\n",
    "for row in predicition_result:\n",
    "    table.add_row(row)\n",
    "\n",
    "table.add_row([5, 'Total', (phase1_predicition + phase2_predicition + phase3_predicition + phase4_predicition)])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información de max phase scaler values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"phase 1 max scaler: {train_test_phase1_scaler.data_max_}\")\n",
    "print(f\"phase 2 max scaler: {train_test_phase2_scaler.data_max_}\")\n",
    "print(f\"phase 3 max scaler: {train_test_phase3_scaler.data_max_}\")\n",
    "print(f\"phase 4 max scaler: {train_test_phase4_scaler.data_max_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
